{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "394a8b44-5290-488b-a29b-d8012aa40c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REMOVE NROWS ARGUMENT FROM DATAFRAMES ITERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966a883b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display cells to maximum width \n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "\n",
    "# lets you preint multiple outputs per cell, not just last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263f3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import time\n",
    "from time import sleep\n",
    "import concurrent\n",
    "import multiprocessing\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pathlib\n",
    "import configparser\n",
    "import sys\n",
    "import urllib\n",
    "import zipfile\n",
    "import csv\n",
    "sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "from pymetamap import MetaMap  # https://github.com/AnthonyMRios/pymetamap/blob/master/pymetamap/SubprocessBackend.py\n",
    "from pandas import ExcelWriter\n",
    "import ast\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import shlex\n",
    "from collections import Counter\n",
    "\n",
    "# %pip install thefuzz\n",
    "# %pip install levenshtein\n",
    "# %pip install xlsxwriter\n",
    "\n",
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37bacfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_sort_ratio(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.token_sort_ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_token_set_ratio(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.token_set_ratio(str1, str2)\n",
    "    except:\n",
    "        return None  \n",
    "\n",
    "def get_similarity_score(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def convert_seconds_to_hms(seconds):\n",
    "    \"\"\" converts the elapsed time or runtime to hours, min, sec \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "def de_ascii_er(text):\n",
    "    non_ascii = \"[^\\x00-\\x7F]\"\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text\n",
    "\n",
    "def start_metamap_servers(metamap_dirs):\n",
    "    global metamap_pos_server_dir\n",
    "    global metamap_wsd_server_dir\n",
    "    metamap_pos_server_dir = 'bin/skrmedpostctl' # Part of speech tagger\n",
    "    metamap_wsd_server_dir = 'bin/wsdserverctl' # Word sense disambiguation \n",
    "    \n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'start']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'start']\n",
    "\n",
    "    # Start servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(5)\n",
    "\n",
    "def stop_metamap_servers(metamap_dirs):\n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'stop']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'stop']\n",
    "    \n",
    "    # Stop servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(2)  \n",
    "    \n",
    "def add_mappings_to_cache(flag_and_path):\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    with open(\"metamapped_terms_cache.tsv\", 'a+', encoding=\"utf-8\") as cache:\n",
    "        with open(f\"{relevant_date}_metamap_output.tsv\", 'r', encoding=\"utf-8\", errors='ignore') as new_metamapped_terms:\n",
    "            # Read the first line from new_metamapped_terms to move the cursor\n",
    "            line = new_metamapped_terms.readline()\n",
    "\n",
    "            # Move the cursor to the position after the first line\n",
    "            while line:\n",
    "                line = new_metamapped_terms.readline()\n",
    "                if line:\n",
    "                    # Append the line to file_1\n",
    "                    cache.write(line)\n",
    "    \"\"\" Remove duplicate rows from cache \"\"\"\n",
    "    cache = pd.read_csv(\"metamapped_terms_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    cache = cache.drop_duplicates()\n",
    "    cache.to_csv('metamapped_terms_cache.tsv', sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "\n",
    "def add_manually_selected_terms_to_cache():\n",
    "    # -----     ------     GENERATE MANUALLY SELECTED CACHE     -----     ------  #\n",
    "    try:\n",
    "        #  --- --- --   CONDITIONS     --- --- --   #\n",
    "        files = glob.glob(\"*.xlsx\")\n",
    "        conditions_manselected_files = [i for i in files if \"conditions_manual_review\" in i if not i.startswith(\"~\")][0]  \n",
    "        conditions_manselected = pd.read_excel(conditions_manselected_files)\n",
    "        conditions_manselected.name.ffill(inplace=True)\n",
    "        conditions_manselected.orig_con.ffill(inplace=True)\n",
    "        conditions_manselected = conditions_manselected[~conditions_manselected['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "        conditions_manselected.drop([\"curie_info\"], axis = 1, inplace = True)\n",
    "        conditions_manselected.rename(columns = {'name':'original_clin_trial_term', 'orig_con':'modified_clin_trial_term'}, inplace = True)\n",
    "\n",
    "        with open('conditions_manually_selected_cache.tsv', 'a') as output:\n",
    "            conditions_manselected.to_csv(output, mode='a',sep=\"\\t\", index=False, header=output.tell()==0)\n",
    "        \"\"\" Remove duplicate rows from cache \"\"\"\n",
    "        cache = pd.read_csv(\"conditions_manually_selected_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        cache = cache.drop_duplicates()\n",
    "        cache.to_csv('conditions_manually_selected_cache.tsv', sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "\n",
    "        #  --- --- --   INTERVENTIONS and Alternate INTERVENTIONS   --- --- --   #\n",
    "        files = glob.glob(\"*.xlsx\")\n",
    "        interventions_manselected_files = [i for i in files if \"interventions_manual_review\" in i if not i.startswith(\"~\")][0]  \n",
    "        interventions_manselected = pd.read_excel(interventions_manselected_files)\n",
    "        interventions_manselected.name.ffill(inplace=True)\n",
    "        interventions_manselected.orig_int.ffill(inplace=True)\n",
    "        interventions_manselected = interventions_manselected[~interventions_manselected['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "        interventions_manselected.drop([\"curie_info\", \"description\"], axis = 1, inplace = True)\n",
    "        interventions_manselected.rename(columns = {'name':'original_clin_trial_term', 'orig_int':'modified_clin_trial_term'}, inplace = True)\n",
    "\n",
    "        with open('interventions_manually_selected_cache.tsv', 'a') as output:\n",
    "            interventions_manselected.to_csv(output, mode='a',sep=\"\\t\", index=False, header=output.tell()==0)\n",
    "        \"\"\" Remove duplicate rows from cache \"\"\"\n",
    "        cache = pd.read_csv(\"interventions_manually_selected_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        cache = cache.drop_duplicates()\n",
    "        cache.to_csv('interventions_manually_selected_cache.tsv', sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "    except:\n",
    "        print(\"No terms in manual select column; either column is empty or bug. Proceeding without them\")\n",
    "        \n",
    "def check_os():\n",
    "    if \"linux\" in sys.platform:\n",
    "        print(\"Linux platform detected\")\n",
    "        metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "        metamap_bin_dir = 'bin/metamap20'\n",
    "    else:\n",
    "        metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "        metamap_bin_dir = 'bin/metamap18'\n",
    "        \n",
    "    return {\"metamap_base_dir\":metamap_base_dir, \"metamap_bin_dir\":metamap_bin_dir}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47c7e3d8-1c7a-44a2-afe5-6721f7ca2438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "\n",
    "    # get all the links and associated dates of upload into a dict called date_link\n",
    "    url_all = \"https://aact.ctti-clinicaltrials.org/download\"\n",
    "    response = requests.get(url_all)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    body = soup.find_all('option') #Find all\n",
    "    date_link = {}\n",
    "    for el in body:\n",
    "        tags = el.find('a')\n",
    "        try:\n",
    "            zip_name = tags.contents[0].split()[0]\n",
    "            date = zip_name.split(\"_\")[0]\n",
    "            date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "            date_link[date] = tags.get('href')\n",
    "        except:\n",
    "            pass\n",
    "    latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "    url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "    date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "    data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "    data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "    data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "\n",
    "    if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "\n",
    "        term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        print(\"Attempting download of Clinical Trial data as of {}\\n\".format(date_string))\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(data_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(\"Finished download of zip\")\n",
    "                with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                    print(\"Unzipping data\")\n",
    "                    download.extractall(data_extracted)\n",
    "        except:\n",
    "            print(\"\\nFailed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\")\n",
    "            print(\"Please store the downloaded zip in the /data directory. This should be the only item besides the cache file, condition manual review file, and intervention manual review file, in the directory at this time.\")\n",
    "            done = input(\"Type Done when done: \")\n",
    "            if done == \"Done\":\n",
    "                data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "                # list_of_files = glob.glob(data_dir + \"/*\") # get all files in directory\n",
    "                try:\n",
    "                    # latest_file = max(list_of_files, key=os.path.getctime) # get the most recent file in the directory\n",
    "                    pattern = os.path.join(data_dir, \"*.zip\")\n",
    "                    zip_file = glob.glob(pattern) # look for file in directory that ends in \".zip\"\n",
    "                    zip_file = zip_file[0]\n",
    "                    print(\"File found at: \")\n",
    "                    print(zip_file)\n",
    "                    # print(latest_file)\n",
    "                    print(\"Please make sure this the correct zip file from AACT\")\n",
    "                    try:\n",
    "                        with zipfile.ZipFile(zip_file, 'r') as download:\n",
    "                            print(\"Unzipping data into\")\n",
    "                            cttime = os.path.getctime(zip_file)\n",
    "                            date_string = dt.datetime.fromtimestamp(cttime).strftime('%m_%d_%Y')\n",
    "                            data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "                            print(data_extracted)\n",
    "                            download.extractall(data_extracted)\n",
    "                    except:\n",
    "                        pattern = os.path.join(data_dir, \"*_extracted\")\n",
    "                        extracted_file = glob.glob(pattern) # look for file in directory that ends in \"_extracted\"\n",
    "                        data_extracted = extracted_file[0]\n",
    "                        extracted_name = os.path.basename(os.path.normpath(extracted_file[0]))\n",
    "                        date_string = extracted_name.replace('_extracted', '')\n",
    "                        print(\"Assuming data is already unzipped\")\n",
    "                        \n",
    "                except:\n",
    "                    print(\"Unable to download and extract Clincal Trial data.\")\n",
    "                    print(\"Cannot find pipe-delimited zip in /data folder.\")\n",
    "    else:\n",
    "        print(\"KG is already up to date.\")\n",
    "\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted, \"date_string\": date_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bccd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_ct_data(flag_and_path, subset_size):\n",
    "    if flag_and_path[\"term_program_flag\"]:\n",
    "        print(\"Exiting program. Assuming KG has already been constructed from most recent data dump from AACT.\")\n",
    "        exit()\n",
    "    else:\n",
    "        data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "        # read in pipe-delimited files \n",
    "        conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_alts_df = pd.read_csv(data_extracted + '/intervention_other_names.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "        if subset_size:   # if a subset size is given, we are running this script on a small subset of the dataset\n",
    "            conditions_df = conditions_df.sample(n=subset_size)\n",
    "            interventions_df = interventions_df.sample(n=subset_size)\n",
    "            interventions_alts_df = interventions_alts_df.sample(n=subset_size)\n",
    "\n",
    "    return {\"conditions\": conditions_df, \"interventions\": interventions_df, \"interventions_alts\": interventions_alts_df}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bea59a-3c2e-46dc-a65d-eff25ce70675",
   "metadata": {},
   "source": [
    "# MAP TERMS TO EXISTING CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff9cfe54-41d0-44d4-975c-9b03ed8828c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def term_list_to_cache(df_dict, flag_and_path):\n",
    "\n",
    "    #  --- --- --     CONDITIONS     --- --- --   #\n",
    "\n",
    "    # retrieve conditions from the new data dump and from the cache, compare, get the diff so as to map just newly encountered terms (conditions)\n",
    "    conditions_list = df_dict['conditions'].name.unique().tolist()\n",
    "    conditions_list = [str(i) for i in conditions_list]\n",
    "    conditions_list = set([i.lower() for i in conditions_list])\n",
    "\n",
    "    # Get CONDITIONS already run through MetaMap (1st cache)\n",
    "    mm_cache_file = \"metamapped_terms_cache.tsv\"\n",
    "    mm_cache_df = pd.read_csv(mm_cache_file, sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    mm_conditions_cache = mm_cache_df[mm_cache_df[\"term_type\"] == \"condition\"]\n",
    "    mm_conditions_cache = mm_conditions_cache['original_clin_trial_term'].unique().tolist()\n",
    "    mm_conditions_cache = list(set([i.lower() for i in mm_conditions_cache]))\n",
    "\n",
    "    # Get CONDITIONS already manually mapped (2nd cache)\n",
    "    manual_cache_file = \"conditions_manually_selected_cache.tsv\"\n",
    "    manual_cache_df = pd.read_csv(manual_cache_file, sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    manual_conditions_cache = manual_cache_df['original_clin_trial_term'].unique().tolist()\n",
    "    manual_conditions_cache = list(set([i.lower() for i in manual_conditions_cache]))\n",
    "\n",
    "    # merge the 2 caches\n",
    "    conditions_cache = mm_conditions_cache + manual_conditions_cache\n",
    "\n",
    "    conditions_new = [x for x in conditions_list if x not in conditions_cache] # find conditions not in the cache (i.g. new conditions to map)\n",
    "    conditions_new = list(filter(None, conditions_new))\n",
    "    conditions_new = [str(i) for i in conditions_new]\n",
    "\n",
    "    #  --- --- --     INTERVENTIONS     --- --- --   #\n",
    "\n",
    "    # retrieve conditions from the new data dump and from the cache, compare, get the diff so as to map just newly encountered terms (interventions)\n",
    "    interventions_list = df_dict['interventions'].name.unique().tolist()\n",
    "    interventions_list = [str(i) for i in interventions_list]\n",
    "    interventions_list = set([i.lower() for i in interventions_list])\n",
    "\n",
    "    # Get INTERVENTIONS already run through MetaMap (1st cache)\n",
    "    mm_cache_file = \"metamapped_terms_cache.tsv\"\n",
    "    mm_cache_df = pd.read_csv(mm_cache_file, sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    mm_interventions_cache = mm_cache_df[mm_cache_df[\"term_type\"] == \"intervention\"]\n",
    "    mm_interventions_cache = mm_interventions_cache['original_clin_trial_term'].unique().tolist()\n",
    "    mm_interventions_cache = list(set([i.lower() for i in mm_interventions_cache]))\n",
    "\n",
    "    # Get INTERVENTIONS already manually mapped (2nd cache)\n",
    "    manual_cache_file = \"interventions_manually_selected_cache.tsv\"\n",
    "    manual_cache_df = pd.read_csv(manual_cache_file, sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    manual_interventions_cache = manual_cache_df['original_clin_trial_term'].unique().tolist()\n",
    "    manual_interventions_cache = list(set([i.lower() for i in manual_interventions_cache]))\n",
    "\n",
    "    # merge the 2 caches\n",
    "    interventions_cache = mm_interventions_cache + manual_interventions_cache\n",
    "\n",
    "    interventions_new = [x for x in interventions_list if x not in interventions_cache] # find conditions not in the cache (i.g. new conditions to map)\n",
    "    interventions_new = list(filter(None, interventions_new))\n",
    "    interventions_new = [str(i) for i in interventions_new]\n",
    "\n",
    "    #  --- --- --     ALTERNATE INTERVENTIONS     --- --- --   #\n",
    "\n",
    "    # retrieve conditions from the new data dump and from the cache, compare, get the diff so as to map just newly encountered terms (interventions)\n",
    "    interventions_alts_list = df_dict['interventions_alts'].name.unique().tolist()\n",
    "    interventions_alts_list = [str(i) for i in interventions_alts_list]\n",
    "    interventions_alts_list = set([i.lower() for i in interventions_alts_list])\n",
    "\n",
    "    # Get ALTERNATE INTERVENTIONS already run through MetaMap (1st cache)\n",
    "    mm_cache_file = \"metamapped_terms_cache.tsv\"\n",
    "    mm_cache_df = pd.read_csv(mm_cache_file, sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    mm_interventions_alts_cache = mm_cache_df[mm_cache_df[\"term_type\"] == \"alternate_intervention\"]\n",
    "    mm_interventions_alts_cache = mm_interventions_alts_cache['original_clin_trial_term'].unique().tolist()\n",
    "    mm_interventions_alts_cache = list(set([i.lower() for i in mm_interventions_alts_cache]))\n",
    "\n",
    "    # Get ALTERNATE INTERVENTIONS already manually mapped (2nd cache)\n",
    "    manual_cache_file = \"interventions_manually_selected_cache.tsv\"\n",
    "    manual_cache_df = pd.read_csv(manual_cache_file, sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    manual_interventions_alts_cache = manual_cache_df['original_clin_trial_term'].unique().tolist()\n",
    "    manual_interventions_alts_cache = list(set([i.lower() for i in manual_interventions_alts_cache]))\n",
    "\n",
    "    # merge the 2 caches\n",
    "    interventions_alts_cache = mm_interventions_alts_cache + manual_interventions_alts_cache\n",
    "\n",
    "    interventions_alts_new = [x for x in interventions_alts_list if x not in interventions_alts_cache] # find conditions not in the cache (i.g. new conditions to map)\n",
    "    interventions_alts_new = list(filter(None, interventions_alts_new))\n",
    "    interventions_alts_new = [str(i) for i in interventions_alts_new]\n",
    "\n",
    "    dict_new_terms = {\"conditions\": conditions_new, \"interventions\": interventions_new, \"interventions_alts\": interventions_alts_new}\n",
    "    return dict_new_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66732624-e3c1-46e0-8e4e-1ed800ab63a5",
   "metadata": {},
   "source": [
    "# USE METAMAP LOCAL TO MAP REMAINING TERMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0ea402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_metamap(orig_term, input_term, params, mm, cond_or_inter, csv_writer):\n",
    "def run_metamap(term_pair, params, mm, cond_or_inter, csv_writer):\n",
    "    \n",
    "    orig_term = term_pair[0]\n",
    "    input_term = term_pair[1]\n",
    "    from_metamap = []\n",
    "    \n",
    "    if params.get(\"exclude_sts\") is None: # exclude_sts is used for Interventions. restrict_to_sts is used for Conditions. So, the logic is, if we're mapping Conditions, execute \"if\" part of code. If we're mapping Interventions, execute \"else\" part of code\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 restrict_to_sts = params[\"restrict_to_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],\n",
    "                                                )\n",
    "            for concept in concepts:\n",
    "                concept_info = []\n",
    "                concept = concept._asdict()\n",
    "                concept_info.extend([cond_or_inter, orig_term, input_term])\n",
    "                concept_info.extend([concept.get(k) for k in ['preferred_name', 'cui', 'score', 'semtypes']])\n",
    "                from_metamap.append(concept_info)\n",
    "        except:\n",
    "            from_metamap.extend([cond_or_inter, orig_term, input_term, None, None, None, None])\n",
    "    else:\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 exclude_sts = params[\"exclude_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],\n",
    "                                                )\n",
    "            for concept in concepts:\n",
    "                concept_info = []\n",
    "                concept = concept._asdict()\n",
    "                concept_info.extend([cond_or_inter, orig_term, input_term])\n",
    "                concept_info.extend([concept.get(k) for k in ['preferred_name', 'cui', 'score', 'semtypes']])\n",
    "                from_metamap.append(concept_info)\n",
    "        except:\n",
    "            from_metamap.extend([cond_or_inter, orig_term, input_term, None, None, None, None])\n",
    "        \n",
    "    for result in from_metamap:\n",
    "        # print(result)\n",
    "        csv_writer.writerow(result)\n",
    "    return from_metamap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a937acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_metamap(term_pair_list, params, cond_or_inter, flag_and_path, csv_writer):\n",
    "    LENGTH = len(term_pair_list)  # Number of iterations required to fill progress bar (pbar)\n",
    "    pbar = tqdm(total=LENGTH, desc=\"% {}s mapped\".format(cond_or_inter), position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "\n",
    "    start_metamap_servers(metamap_dirs) # start the MetaMap servers\n",
    "    mm = MetaMap.get_instance(metamap_dirs[\"metamap_base_dir\"] + metamap_dirs[\"metamap_bin_dir\"])\n",
    "    with concurrent.futures.ThreadPoolExecutor((multiprocessing.cpu_count()*2) - 1) as executor:\n",
    "        futures = [executor.submit(run_metamap, term_pair, params, mm, cond_or_inter, csv_writer) for term_pair in term_pair_list]\n",
    "        for _ in concurrent.futures.as_completed(futures):\n",
    "            pbar.update(n=1)  # Increments counter\n",
    "    stop_metamap_servers(metamap_dirs) # stop the MetaMap servers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f37ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_list_to_mm(dict_new_terms, flag_and_path):   \n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    deasciier = np.vectorize(de_ascii_er) # vectorize function\n",
    "\n",
    "    # prep output file of Metamap results\n",
    "    filename = f\"{relevant_date}_metamap_output.tsv\"\n",
    "    metamap_output = open(filename, 'w+', newline='')\n",
    "    col_names = ['term_type', 'original_clin_trial_term', 'modified_clin_trial_term', 'metamap_preferred_name', 'metamap_cui', 'metamap_score', 'metamap_semantic_type']\n",
    "    csv_writer = csv.writer(metamap_output, delimiter='\\t')\n",
    "    csv_writer.writerow(col_names)\n",
    "\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "\n",
    "    orig_cons = dict_new_terms.get(\"conditions\")\n",
    "    condition_params = {\"restrict_to_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "\n",
    "    orig_ints = dict_new_terms.get(\"interventions\")\n",
    "    intervention_params = {\"exclude_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "\n",
    "    orig_int_alts = dict_new_terms.get(\"interventions_alts\")\n",
    "    intervention_alts_params = intervention_params\n",
    "\n",
    "    if metamap_version[0] >= 20:\n",
    "        print(\"MetaMap version >= 2020, conduct mapping on original terms\")\n",
    "        # parallelize_metamap(orig_cons, condition_params, \"condition\", flag_and_path, csv_writer)\n",
    "        parallelize_metamap(list(zip(orig_cons, orig_cons)), condition_params, \"condition\", flag_and_path, csv_writer)\n",
    "        parallelize_metamap(list(zip(orig_ints, orig_ints)), intervention_params, \"intervention\", flag_and_path, csv_writer)\n",
    "        parallelize_metamap(list(zip(orig_int_alts, orig_int_alts)), intervention_alts_params, \"alternate_intervention\", flag_and_path, csv_writer)\n",
    "    else:\n",
    "        print(\"MetaMap version < 2020, conduct mapping on terms after removing ascii characters\")\n",
    "        deascii_cons = deasciier(orig_cons)\n",
    "        deascii_ints = deasciier(orig_ints)\n",
    "        deascii_int_alts = deasciier(orig_int_alts)\n",
    "        parallelize_metamap(list(zip(orig_cons, deascii_cons)), condition_params, \"condition\", flag_and_path, csv_writer)\n",
    "        parallelize_metamap(list(zip(orig_ints, deascii_ints)), intervention_params, \"intervention\", flag_and_path, csv_writer)\n",
    "        parallelize_metamap(list(zip(orig_int_alts, deascii_int_alts)), intervention_alts_params, \"alternate_intervention\", flag_and_path, csv_writer)\n",
    "\n",
    "    metamap_output.close()\n",
    "    add_mappings_to_cache(flag_and_path)\n",
    "    add_manually_selected_terms_to_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35a0e308-ce1e-4958-a390-76fe97336a36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mapping UMLS CURIEs and names back to clinical trials\n"
     ]
    }
   ],
   "source": [
    "def map_to_trial(flag_and_path):\n",
    "    print(\"\\nMapping UMLS CURIEs and names back to clinical trials\")\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "\n",
    "    metamap_input = \"metamapped_terms_cache.tsv\"\n",
    "    metamapped = pd.read_csv(metamap_input, sep='\\t', index_col=False, header=0, encoding=\"utf-8\", on_bad_lines = 'warn')\n",
    "\n",
    "    metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", on_bad_lines = 'warn') # get the full names of the semantic types so we know what we're looking at\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].str.replace(r'\\[|\\]', '', regex=True)\n",
    "    sem_type_col_names = [\"abbv\", \"group\", \"semantic_type_full\"]\n",
    "    metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", sep=\"|\", index_col=False, header=None, names=sem_type_col_names, on_bad_lines = 'warn')\n",
    "    sem_type_dict = dict(zip(metamap_semantic_types['abbv'], metamap_semantic_types['semantic_type_full'])) # make a dict of semantic type abbv and full name\n",
    "\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: x.split(',') if isinstance(x, str) else np.nan) # Handle NaN (None) values in metamap_semantic_type column\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: '|'.join([sem_type_dict[term] if term in sem_type_dict else term for term in x]) if isinstance(x, list) else x) # map semantic type abbreviations to the full name of the semantic type\n",
    "\n",
    "    metamapped['metamap_preferred_name'] = metamapped['metamap_preferred_name'].str.lower()\n",
    "    metamapped = metamapped.dropna(axis=0)\n",
    "    metamapped = metamapped[[\"term_type\", \"original_clin_trial_term\", \"modified_clin_trial_term\", \"metamap_cui\",\"metamap_preferred_name\", \"metamap_semantic_type\"]]\n",
    "\n",
    "    metamapped[\"metamap_term_info\"] = metamapped[[\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"]].values.tolist() \n",
    "    metamapped.drop([\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"], axis = 1, inplace = True)\n",
    "    metamapped = metamapped.groupby(['term_type', 'original_clin_trial_term'])['metamap_term_info'].agg(list).reset_index()\n",
    "\n",
    "    data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "    # read in pipe-delimited files \n",
    "    conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    conditions_mapped = conditions_df.copy()\n",
    "\n",
    "    metamapped_con = metamapped.loc[metamapped['term_type'] == \"condition\"]\n",
    "    mapper_con = dict(zip(metamapped_con['original_clin_trial_term'], metamapped_con['metamap_term_info'])) # make a dict from the metamapped cache to map conditions\n",
    "    conditions_mapped['curie_info'] = conditions_mapped['downcase_name'].map(mapper_con)\n",
    "\n",
    "    data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "    # read in pipe-delimited files \n",
    "    interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    interventions_mapped = interventions_df.copy()\n",
    "    interventions_mapped[\"downcase_name\"] = interventions_mapped['name'].str.lower()\n",
    "\n",
    "    metamapped_int = metamapped.loc[(metamapped['term_type'] == \"intervention\") | (metamapped['term_type'] == \"alternate_intervention\")]\n",
    "    mapper_int = dict(zip(metamapped_int['original_clin_trial_term'], metamapped_int['metamap_term_info'])) # make a dict from the metamapped cache to map interventions\n",
    "    interventions_mapped['curie_info'] = interventions_mapped['downcase_name'].map(mapper_int)\n",
    "\n",
    "    interventions_alts_df = pd.read_csv(data_extracted + '/intervention_other_names.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    interventions_alts_mapped = interventions_alts_df.copy()\n",
    "    interventions_alts_mapped.drop([\"id\"], axis=1, inplace=True)\n",
    "    interventions_alts_mapped = interventions_alts_mapped.merge(interventions_df[[\"id\", \"intervention_type\", \"description\"]], left_on='intervention_id', right_on='id', how='left') \n",
    "    interventions_alts_mapped.drop([\"id\"], axis=1, inplace=True)\n",
    "    interventions_alts_mapped[\"downcase_name\"] = interventions_alts_mapped['name'].str.lower()\n",
    "\n",
    "    metamapped_int = metamapped.loc[(metamapped['term_type'] == \"intervention\") | (metamapped['term_type'] == \"alternate_intervention\")]\n",
    "    mapper_int = dict(zip(metamapped_int['original_clin_trial_term'], metamapped_int['metamap_term_info'])) # make a dict from the metamapped cache to map interventions\n",
    "    interventions_alts_mapped['curie_info'] = interventions_alts_mapped['downcase_name'].map(mapper_int)\n",
    "    interventions_alts_mapped = interventions_alts_mapped[[\"intervention_id\", \"nct_id\", \"intervention_type\", \"name\", \"description\", \"downcase_name\", \"curie_info\"]]\n",
    "\n",
    "    # # # conditions_actually_mapped = conditions_mapped[~conditions_mapped['curie_info'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "    # # # interventions_actually_mapped = interventions_mapped[~interventions_mapped['curie_info'].isnull()] # check if the interventions got mapped to any CURIEs\n",
    "    # # # interventions_alts_actually_mapped = interventions_alts_mapped[~interventions_alts_mapped['curie_info'].isnull()] # check if the interventions got mapped to any CURIEs\n",
    "\n",
    "    conditions_mapped.to_csv('{}_conditions_mapped.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output conditions to TSV\n",
    "    interventions_mapped.to_csv('{}_interventions_mapped.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n",
    "    interventions_alts_mapped.to_csv('{}_interventions_alternates_mapped.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output alternate interventions to TSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73da4ec1-ebfc-467c-a0c7-667ee0af93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_mappings(flag_and_path):\n",
    "\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    pattern_outside = r'(?<=\\().+?(?=\\))|([^(]+)'\n",
    "    pattern_inside = r'\\(([^)]+)\\)'\n",
    "\n",
    "    # -----     ------     CONDITIONS     -----     ------  #\n",
    "\n",
    "    print(\"Scoring mappings for CONDITIONS\")\n",
    "    \n",
    "    # prep output file of condition scored results\n",
    "    filename = f\"{relevant_date}_conditions_scored.tsv\"\n",
    "    conditions_scored_output = open(filename, 'w+', newline='')\n",
    "    col_names = [\"condition_id\", \"nct_id\", \"name\", \"orig_con\", \"curie_info\", \"orig_con_outside\", \"orig_con_inside\"]\n",
    "    csv_writer = csv.writer(conditions_scored_output, delimiter='\\t')\n",
    "    csv_writer.writerow(col_names)\n",
    "\n",
    "    with pd.read_csv(\"{}_conditions_mapped.tsv\".format(relevant_date), sep='\\t', index_col=False, header=0, on_bad_lines = 'warn', chunksize=1000, nrows=4000) as reader:\n",
    "        for conditions_df_chunk in reader:\n",
    "            conditions = conditions_df_chunk.copy()\n",
    "            conditions.rename(columns = {'downcase_name':'orig_con','id': 'condition_id'}, inplace = True)\n",
    "\n",
    "            matches_outside = conditions['orig_con'].str.extract(pattern_outside)\n",
    "            conditions['orig_con_outside'] = matches_outside[0].fillna(np.nan).replace([np.nan], [None]) # if no matches inside ()/to regex pattern, then fill with None\n",
    "            matches_inside = conditions['orig_con'].str.extract(pattern_inside)\n",
    "            conditions['orig_con_inside'] = matches_inside[0].fillna(np.nan).replace([np.nan], [None]) # # if no matches outside ()/to regex pattern, then fill with None\n",
    "\n",
    "            conditions = conditions.fillna(np.nan).replace([np.nan], [None]) # replace NaN values in dataframe with None (just for consistency)\n",
    "            conditions_cols = conditions.columns.tolist() # get all column names as list so we can dynamically change column values as we iterate through rows \n",
    "\n",
    "            cols_to_check = [ele for ele in conditions.columns if any([substr in ele for substr in ['_con']])] # find columns with \"con\" in it \n",
    "\n",
    "            for i, row in conditions.iterrows():\n",
    "                orig_row = row\n",
    "                new_row = orig_row.mask(orig_row.duplicated(), None) # if term/condition inside or outside parentheses is duplicated, replace the duplicate term with None so we don't waste time scoring it\n",
    "                conditions.loc[i, conditions_cols] = new_row \n",
    "                curies_sublists_scored = []\n",
    "                for col_name in cols_to_check: # check only columns with terms to score\n",
    "                    value = row[col_name]\n",
    "                    curie_info = row[\"curie_info\"]\n",
    "                    if None not in [value, curie_info]: # if the column has a term to score in that row...\n",
    "                        curie_sublists = ast.literal_eval(curie_info)\n",
    "                        for sublist in curie_sublists:\n",
    "                            sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}') # get the sort ratio score for that term and the CURIE from MetaMap\n",
    "                            sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}') # get the similarity score for that term and the CURIE from MetaMap\n",
    "                            curies_sublists_scored.append(sublist)\n",
    "                if curies_sublists_scored:\n",
    "                    curies_sublists_scored = [list(y) for y in set([tuple(x) for x in curies_sublists_scored])] # remove any duplicate sublists\n",
    "                    conditions.at[i, \"curie_info\"] = curies_sublists_scored\n",
    "                else:\n",
    "                    conditions.at[i, \"curie_info\"] = None\n",
    "                scored_row = conditions.loc[i, :].values.tolist()\n",
    "                scored_row = list(map(lambda x: str(x) if x is not None else \"\", scored_row))\n",
    "                csv_writer.writerow(scored_row)                \n",
    "    conditions_scored_output.close() \n",
    "\n",
    "    # # -----     ------     INTERVENTIONS     -----     ------  #\n",
    "\n",
    "    print(\"Scoring mappings for INTERVENTIONS\")\n",
    "\n",
    "    # prep output file of intervention scored results\n",
    "    filename = f\"{relevant_date}_interventions_scored.tsv\"\n",
    "    interventions_scored_output = open(filename, 'w+', newline='')\n",
    "    col_names = [\"intervention_id\", \"nct_id\", \"intervention_type\", \"name\", \"description\", \"orig_int\", \"curie_info\", \"orig_int_inside\", \"orig_int_outside\"] \n",
    "    csv_writer = csv.writer(interventions_scored_output, delimiter='\\t')\n",
    "    csv_writer.writerow(col_names)\n",
    "\n",
    "    with pd.read_csv(\"{}_interventions_mapped.tsv\".format(relevant_date), sep='\\t', index_col=False, header=0, on_bad_lines = 'warn', chunksize=1000, nrows=4000) as reader:\n",
    "        for interventions_df_chunk in reader:\n",
    "            interventions = interventions_df_chunk.copy()\n",
    "            interventions.rename(columns = {'downcase_name':'orig_int','id': 'intervention_id'}, inplace = True)\n",
    "\n",
    "            matches_outside = interventions['orig_int'].str.extract(pattern_outside)\n",
    "            interventions['orig_int_outside'] = matches_outside[0].fillna(np.nan).replace([np.nan], [None]) # if no matches inside ()/to regex pattern, then fill with None\n",
    "            matches_inside = interventions['orig_int'].str.extract(pattern_inside)\n",
    "            interventions['orig_int_inside'] = matches_inside[0].fillna(np.nan).replace([np.nan], [None]) # # if no matches outside ()/to regex pattern, then fill with None\n",
    "\n",
    "            interventions = interventions.fillna(np.nan).replace([np.nan], [None]) # replace NaN values in dataframe with None (just for consistency)\n",
    "            interventions_cols = interventions.columns.tolist() # get all column names as list so we can dynamically change column values as we iterate through rows \n",
    "\n",
    "            cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])] # find columns with \"int\" in it \n",
    "\n",
    "            for i, row in interventions.iterrows():\n",
    "                orig_row = row\n",
    "                new_row = orig_row.mask(orig_row.duplicated(), None) # if term/condition inside or outside parentheses is duplicated, replace the duplicate term with None so we don't waste time scoring it\n",
    "                interventions.loc[i, interventions_cols] = new_row \n",
    "                curies_sublists_scored = []\n",
    "                for col_name in cols_to_check: # check only columns with terms to score\n",
    "                    value = row[col_name]\n",
    "                    curie_info = row[\"curie_info\"]\n",
    "                    if None not in [value, curie_info]: # if the column has a term to score in that row...\n",
    "                        curie_sublists = ast.literal_eval(curie_info)\n",
    "                        for sublist in curie_sublists:\n",
    "                            sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}') # get the sort ratio score for that term and the CURIE from MetaMap\n",
    "                            sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}') # get the similarity score for that term and the CURIE from MetaMap\n",
    "                            curies_sublists_scored.append(sublist)\n",
    "                if curies_sublists_scored:\n",
    "                    curies_sublists_scored = [list(y) for y in set([tuple(x) for x in curies_sublists_scored])] # remove any duplicate sublists\n",
    "                    interventions.at[i, \"curie_info\"] = curies_sublists_scored\n",
    "                else:\n",
    "                    interventions.at[i, \"curie_info\"] = None\n",
    "                scored_row = interventions.loc[i, :].values.tolist()\n",
    "                scored_row = list(map(lambda x: str(x) if x is not None else \"\", scored_row)) \n",
    "                csv_writer.writerow(scored_row)\n",
    "    interventions_scored_output.close()\n",
    "    \n",
    "    # # -----     ------     ALTERNATE INTERVENTIONS     -----     ------  #\n",
    "    # prep output file of intervention scored results\n",
    "    filename = f\"{relevant_date}_interventions_alternates_scored.tsv\"\n",
    "    interventions_alts_scored_output = open(filename, 'w+', newline='')\n",
    "    col_names = [\"intervention_id\", \"nct_id\", \"intervention_type\", \"name\", \"description\", \"orig_int_alt\", \"curie_info\", \"orig_int_alt_inside\", \"orig_int_alt_outside\"] \n",
    "\n",
    "    csv_writer = csv.writer(interventions_alts_scored_output, delimiter='\\t')\n",
    "    csv_writer.writerow(col_names)\n",
    "\n",
    "    with pd.read_csv(\"{}_interventions_alternates_mapped.tsv\".format(relevant_date), sep='\\t', index_col=False, header=0, on_bad_lines = 'warn', chunksize=1000, nrows=4000) as reader:\n",
    "        for interventions_df_chunk in reader:\n",
    "            interventions = interventions_df_chunk.copy()\n",
    "            interventions.rename(columns = {'downcase_name':'orig_int_alt'}, inplace = True)\n",
    "\n",
    "            matches_outside = interventions['orig_int_alt'].str.extract(pattern_outside)\n",
    "            interventions['orig_int_alt_outside'] = matches_outside[0].fillna(np.nan).replace([np.nan], [None]) # if no matches inside ()/to regex pattern, then fill with None\n",
    "            matches_inside = interventions['orig_int_alt'].str.extract(pattern_inside)\n",
    "            interventions['orig_int_alt_inside'] = matches_inside[0].fillna(np.nan).replace([np.nan], [None]) # # if no matches outside ()/to regex pattern, then fill with None\n",
    "\n",
    "            interventions = interventions.fillna(np.nan).replace([np.nan], [None]) # replace NaN values in dataframe with None (just for consistency)\n",
    "            interventions_cols = interventions.columns.tolist() # get all column names as list so we can dynamically change column values as we iterate through rows \n",
    "\n",
    "            cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])] # find columns with \"int\" in it \n",
    "\n",
    "            for i, row in interventions.iterrows():\n",
    "                orig_row = row\n",
    "                new_row = orig_row.mask(orig_row.duplicated(), None) # if term/condition inside or outside parentheses is duplicated, replace the duplicate term with None so we don't waste time scoring it\n",
    "                interventions.loc[i, interventions_cols] = new_row \n",
    "                curies_sublists_scored = []\n",
    "                for col_name in cols_to_check: # check only columns with terms to score\n",
    "                    value = row[col_name]\n",
    "                    curie_info = row[\"curie_info\"]\n",
    "                    if None not in [value, curie_info]: # if the column has a term to score in that row...\n",
    "                        curie_sublists = ast.literal_eval(curie_info)\n",
    "                        for sublist in curie_sublists:\n",
    "                            sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}') # get the sort ratio score for that term and the CURIE from MetaMap\n",
    "                            sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}') # get the similarity score for that term and the CURIE from MetaMap\n",
    "                            curies_sublists_scored.append(sublist)\n",
    "                if curies_sublists_scored:\n",
    "                    curies_sublists_scored = [list(y) for y in set([tuple(x) for x in curies_sublists_scored])] # remove any duplicate sublists\n",
    "                    interventions.at[i, \"curie_info\"] = curies_sublists_scored\n",
    "                else:\n",
    "                    interventions.at[i, \"curie_info\"] = None\n",
    "                scored_row = interventions.loc[i, :].values.tolist()\n",
    "                scored_row = list(map(lambda x: str(x) if x is not None else \"\", scored_row)) \n",
    "                csv_writer.writerow(scored_row)\n",
    "    interventions_scored_output.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0bd74b18-d468-4c9d-88fc-502cb832f4e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auto_select_curies(flag_and_path):\n",
    "    print(\"Auto-selecting high scoring CURIEs\")\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "\n",
    "    # select sublist with highest scoring term\n",
    "    def filter_and_select_sublist(sublists):  # function to find the highest score of a CURIE, and pick that curie if it's greater than threshold of 88\n",
    "        try:\n",
    "            selected_sublist = None\n",
    "            if pd.isnull(sublists):\n",
    "                return selected_sublist\n",
    "            else:\n",
    "                high_score = -1\n",
    "                sublists = ast.literal_eval(sublists)\n",
    "                for sublist in sublists:\n",
    "                    if len(sublist) >= 4:\n",
    "                        sort_ratio = int(sublist[3].split(\": \")[1])\n",
    "                        sim_score = int(sublist[4].split(\": \")[1])\n",
    "                        max_score = max(sort_ratio, sim_score)\n",
    "                        if max_score > high_score:\n",
    "                            high_score  = max_score\n",
    "                            selected_sublist = sublist\n",
    "                        else:\n",
    "                            continue\n",
    "                        if max_score < 80:\n",
    "                            selected_sublist = None\n",
    "            return selected_sublist\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    # convert string to list, return empty list\n",
    "    def convert_to_list(x):\n",
    "        try:\n",
    "            listx = ast.literal_eval(x)\n",
    "            return listx\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            return []\n",
    "\n",
    "    # # -----     ------     CONDITIONS     -----     ------  #\n",
    "\n",
    "    # if the previous autoselected or dump of \"no CURIE selected\" files exist, delete them\n",
    "    if os.path.exists(f'{relevant_date}_conditions_autoselected.tsv'):\n",
    "        os.remove(f'{relevant_date}_conditions_autoselected.tsv')\n",
    "    if os.path.exists(f'{relevant_date}_conditions_manual_review.tsv'):\n",
    "        os.remove(f'{relevant_date}_conditions_manual_review.tsv')\n",
    "\n",
    "    with pd.read_csv(f\"{relevant_date}_conditions_scored.tsv\", sep='\\t', usecols=[\"condition_id\", \"nct_id\", \"name\", \"orig_con\", \"curie_info\"], index_col=False, header=0, on_bad_lines = 'warn', chunksize=1000, nrows=4000) as reader:\n",
    "        print(\"Auto-selecting CONDITIONS CURIEs\")\n",
    "        write_header = True\n",
    "\n",
    "        for chunk_df in reader:\n",
    "            \"\"\"  Create an output TSV of CURIEs that are auto-selected based on passing the threshold of scoring > 80  \"\"\"\n",
    "            chunk_df['auto_selected_curie'] = chunk_df['curie_info'].apply(filter_and_select_sublist)  # select CURIE that scores highest using filter_and_select_sublist function = auto-select\n",
    "            auto_selected = chunk_df.loc[chunk_df['auto_selected_curie'].notnull(),]  # get the rows where a CURIE has been auto-selected\n",
    "            auto_selected.to_csv(f'{relevant_date}_conditions_autoselected.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a') # output to TSV\n",
    "\n",
    "            manual_review = chunk_df.loc[~chunk_df['auto_selected_curie'].notnull(),] # get the rows where a CURIE has not been selected or MetaMap did not return a term\n",
    "            manual_review = manual_review[[\"name\", \"orig_con\", \"curie_info\", \"auto_selected_curie\"]]\n",
    "            manual_review = manual_review.drop_duplicates() # run this on the chunked df just bc...we repeat this on the entire dataframe later\n",
    "            manual_review.to_csv(f'{relevant_date}_conditions_manual_review.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a') # output to TSV\n",
    "\n",
    "            write_header = False  \n",
    "\n",
    "    \"\"\"  Create an output TSV of possible CURIEs available for each term that was not auto-selected  \"\"\"\n",
    "    manual_review = pd.read_csv(f'{relevant_date}_conditions_manual_review.tsv', sep=\"\\t\", on_bad_lines = 'warn')\n",
    "    manual_review.drop([\"auto_selected_curie\"], axis = 1, inplace = True)   # drop the autoselect column bc it's empty, these rows are specifically ones where nothing was selected\n",
    "    manual_review = manual_review.drop_duplicates()\n",
    "    manual_review.loc[:, \"curie_info\"] = manual_review.curie_info.apply(lambda x: convert_to_list(x)) # in order to multi-index, we have to group-by the original input term. To do this, first convert the curie_info column to list of lists\n",
    "    manual_review = manual_review.explode('curie_info')  # explode that column so every sublist is on a separate row\n",
    "    manual_review['curie_info'] = manual_review['curie_info'].apply(lambda x: x[:3] if isinstance(x, list) else None)   # remove the scores (sort_ratio and similarity score) from the list, don't need them and they compromise readability of manual outputs \n",
    "    manual_review['curie_info'] = manual_review['curie_info'].apply(lambda x: '|--|'.join(x) if isinstance(x, list) else None)  # Multindexing does not work on lists, so remove the CURIE information out of the list to enable this\n",
    "\n",
    "    manual_review['temp'] = \"temp\"   # create a temp column to facilitate multi-indexing\n",
    "    manual_review.set_index([\"name\", \"orig_con\", \"curie_info\"], inplace=True)   # create index\n",
    "    manual_review.drop([\"temp\"], axis = 1, inplace = True)   # drop the temp column\n",
    "    manual_review['manually_selected_CURIE'] = None # make a column \n",
    "\n",
    "    manual_review.to_excel('{}_conditions_manual_review.xlsx'.format(relevant_date), engine='xlsxwriter', index=True)\n",
    "\n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):  # more options can be specified also\n",
    "    #     display(manual_review[:2000])        \n",
    "\n",
    "    # # # -----     ------     INTERVENTIONS     -----     ------  #\n",
    "\n",
    "    # if the previous autoselected or dump of \"no CURIE selected\" files exist, delete them\n",
    "    if os.path.exists(f'{relevant_date}_interventions_autoselected.tsv'):\n",
    "        os.remove(f'{relevant_date}_interventions_autoselected.tsv')\n",
    "    if os.path.exists(f'{relevant_date}_interventions_manual_review.tsv'):\n",
    "        os.remove(f'{relevant_date}_interventions_manual_review.tsv')\n",
    "\n",
    "    with pd.read_csv(f\"{relevant_date}_interventions_scored.tsv\", sep='\\t', usecols=[\"intervention_id\", \"description\", \"nct_id\", \"name\", \"orig_int\", \"curie_info\"], index_col=False, header=0, on_bad_lines = 'warn', chunksize=1000, nrows=4000) as reader:\n",
    "        print(\"Auto-selecting INTERVENTIONS CURIEs\")\n",
    "        write_header = True\n",
    "\n",
    "        for chunk_df in reader:\n",
    "            \"\"\"  Create an output TSV of CURIEs that are auto-selected based on passing the threshold of scoring > 80  \"\"\"\n",
    "            chunk_df['auto_selected_curie'] = chunk_df['curie_info'].apply(filter_and_select_sublist)  # select CURIE that scores highest using filter_and_select_sublist function = auto-select\n",
    "            auto_selected = chunk_df.loc[chunk_df['auto_selected_curie'].notnull(), ]  # get the rows where a CURIE has been auto-selected\n",
    "            auto_selected.to_csv(f'{relevant_date}_interventions_autoselected.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a') # output to TSV\n",
    "\n",
    "            manual_review = chunk_df.loc[~chunk_df['auto_selected_curie'].notnull(),] # get the rows where a CURIE has not been selected or MetaMap did not return a term\n",
    "            manual_review = manual_review[[\"name\", \"orig_int\", \"curie_info\", \"auto_selected_curie\", \"description\"]]\n",
    "            manual_review = manual_review.drop_duplicates([\"orig_int\", \"description\"]) # run this on the chunked df just bc...we repeat this on the entire dataframe later\n",
    "            manual_review.to_csv(f'{relevant_date}_interventions_manual_review.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a') # output to TSV\n",
    "\n",
    "            write_header = False  \n",
    "\n",
    "    \"\"\"  Create an output TSV of possible CURIEs available for each term that was not auto-selected  \"\"\"\n",
    "    manual_review = pd.read_csv(f'{relevant_date}_interventions_manual_review.tsv', sep=\"\\t\", on_bad_lines = 'warn')\n",
    "    manual_review.drop([\"auto_selected_curie\"], axis = 1, inplace = True)   # drop the autoselect column bc it's empty, these rows are specifically ones where nothing was selected\n",
    "    manual_review = manual_review.drop_duplicates()\n",
    "    manual_review.loc[:, \"curie_info\"] = manual_review.curie_info.apply(lambda x: convert_to_list(x)) # in order to multi-index, we have to group-by the original input term. To do this, first convert the curie_info column to list of lists\n",
    "    manual_review = manual_review.explode('curie_info')  # explode that column so every sublist is on a separate row\n",
    "    manual_review['curie_info'] = manual_review['curie_info'].apply(lambda x: x[:3] if isinstance(x, list) else None)   # remove the scores (sort_ratio and similarity score) from the list, don't need them and they compromise readability of manual outputs \n",
    "    manual_review['curie_info'] = manual_review['curie_info'].apply(lambda x: '|--|'.join(x) if isinstance(x, list) else None)  # Multindexing does not work on lists, so remove the CURIE information out of the list to enable this\n",
    "\n",
    "    manual_review['temp'] = \"temp\"   # create a temp column to facilitate multi-indexing\n",
    "    manual_review.set_index([\"name\", \"description\", \"orig_int\", \"curie_info\"], inplace=True)   # create index\n",
    "    manual_review.drop([\"temp\"], axis = 1, inplace = True)   # drop the temp column\n",
    "    manual_review['manually_selected_CURIE'] = None # make a column \n",
    "\n",
    "    manual_review.to_excel('{}_interventions_manual_review.xlsx'.format(relevant_date), engine='xlsxwriter', index=True)  \n",
    "    \n",
    "    \n",
    "    # # # -----     ------     ALTERNATE INTERVENTIONS     -----     ------  #\n",
    "\n",
    "    # if the previous autoselected or dump of \"no CURIE selected\" files exist, delete them\n",
    "    if os.path.exists(f'{relevant_date}_interventions_alternates_autoselected.tsv'):\n",
    "        os.remove(f'{relevant_date}_interventions_alternates_autoselected.tsv')\n",
    "    if os.path.exists(f'{relevant_date}_interventions_alternates_manual_review.tsv'):\n",
    "        os.remove(f'{relevant_date}_interventions_alternates_manual_review.tsv')\n",
    "\n",
    "    with pd.read_csv(f\"{relevant_date}_interventions_alternates_scored.tsv\", sep='\\t', usecols=[\"intervention_id\", \"description\", \"nct_id\", \"name\", \"orig_int_alt\", \"curie_info\"], index_col=False, header=0, on_bad_lines = 'warn', chunksize=1000, nrows=4000) as reader:\n",
    "        print(\"Auto-selecting ALTERNATE INTERVENTIONS CURIEs\")\n",
    "        write_header = True\n",
    "\n",
    "        for chunk_df in reader:\n",
    "            \"\"\"  Create an output TSV of CURIEs that are auto-selected based on passing the threshold of scoring > 80  \"\"\"\n",
    "            chunk_df['auto_selected_curie'] = chunk_df['curie_info'].apply(filter_and_select_sublist)  # select CURIE that scores highest using filter_and_select_sublist function = auto-select\n",
    "            auto_selected = chunk_df.loc[chunk_df['auto_selected_curie'].notnull(), ]  # get the rows where a CURIE has been auto-selected\n",
    "            auto_selected.to_csv(f'{relevant_date}_interventions_alternates_autoselected.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a') # output to TSV\n",
    "\n",
    "            manual_review = chunk_df.loc[~chunk_df['auto_selected_curie'].notnull(),] # get the rows where a CURIE has not been selected or MetaMap did not return a term\n",
    "            manual_review = manual_review[[\"name\", \"orig_int_alt\", \"curie_info\", \"auto_selected_curie\", \"description\"]]\n",
    "            manual_review = manual_review.drop_duplicates([\"orig_int_alt\", \"description\"]) # run this on the chunked df just bc...we repeat this on the entire dataframe later\n",
    "            manual_review.to_csv(f'{relevant_date}_interventions_alternates_manual_review.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a') # output to TSV\n",
    "\n",
    "            write_header = False  \n",
    "\n",
    "    \"\"\"  Create an output TSV of possible CURIEs available for each term that was not auto-selected  \"\"\"\n",
    "    manual_review = pd.read_csv(f'{relevant_date}_interventions_alternates_manual_review.tsv', sep=\"\\t\", on_bad_lines = 'warn')\n",
    "    manual_review.drop([\"auto_selected_curie\"], axis = 1, inplace = True)   # drop the autoselect column bc it's empty, these rows are specifically ones where nothing was selected\n",
    "    manual_review = manual_review.drop_duplicates()\n",
    "    manual_review.loc[:, \"curie_info\"] = manual_review.curie_info.apply(lambda x: convert_to_list(x)) # in order to multi-index, we have to group-by the original input term. To do this, first convert the curie_info column to list of lists\n",
    "    manual_review = manual_review.explode('curie_info')  # explode that column so every sublist is on a separate row\n",
    "    manual_review['curie_info'] = manual_review['curie_info'].apply(lambda x: x[:3] if isinstance(x, list) else None)   # remove the scores (sort_ratio and similarity score) from the list, don't need them and they compromise readability of manual outputs \n",
    "    manual_review['curie_info'] = manual_review['curie_info'].apply(lambda x: '|--|'.join(x) if isinstance(x, list) else None)  # Multindexing does not work on lists, so remove the CURIE information out of the list to enable this\n",
    "\n",
    "    manual_review['temp'] = \"temp\"   # create a temp column to facilitate multi-indexing\n",
    "    manual_review.set_index([\"name\", \"description\", \"orig_int_alt\", \"curie_info\"], inplace=True)   # create index\n",
    "    manual_review.drop([\"temp\"], axis = 1, inplace = True)   # drop the temp column\n",
    "    manual_review['manually_selected_CURIE'] = None # make a column \n",
    "\n",
    "    manual_review.to_excel('{}_interventions_alternates_manual_review.xlsx'.format(relevant_date), engine='xlsxwriter', index=True)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6460d8cf-76d8-4e9d-95d8-10a5cfdddea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ETL(subset_size):\n",
    "\n",
    "    start_time_begin = time.time()\n",
    "    flag_and_path = get_raw_ct_data() # download raw data\n",
    "    end_time_download = time.time()\n",
    "    elapsed_time = end_time_download - start_time_begin\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "    print(f\"\\nApproximate runtime for downloading or locating raw data: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "\n",
    "    global metamap_dirs\n",
    "    metamap_dirs = check_os()\n",
    "    df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "    dict_new_terms = term_list_to_cache(df_dict, flag_and_path) # use the existing cache of MetaMapped terms so that only new terms are mapped\n",
    "\n",
    "    start_time_mm = time.time()\n",
    "    term_list_to_mm(dict_new_terms, flag_and_path) # map new terms using MetaMap\n",
    "    end_time_mm = time.time()\n",
    "    elapsed_time = end_time_mm - start_time_mm\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "    print(f\"Approximate runtime for mapping: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "\n",
    "    map_to_trial(flag_and_path) # map MetaMap terms back to trial \n",
    "    score_mappings(flag_and_path) # score the mappings\n",
    "    auto_select_curies(flag_and_path) # select CURIEs automatically that pass score threshold\n",
    "    \n",
    "    # compile_curies_for_trials(flag_and_path) # select CURIEs automatically that pass score threshold\n",
    "\n",
    "    end_time_end = time.time()\n",
    "    elapsed_time = end_time_end - start_time_begin\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "    print(f\"Approximate runtime for overall mapping: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6219e63a-4b74-4db2-8c9f-ed4c8326b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_or_prod():\n",
    "    global subset_size\n",
    "    subset_size = 100\n",
    "    print(f\"The test run of this code performs the construction of the KG on a random subset of {subset_size} Conditions, {subset_size} Interventions, and {subset_size} Alternate Interventions from Clinical Trials.\")\n",
    "    test_or_prod = input(\"Is this a test run or the production of a new version of the KG? Enter Test for test, or Prod for production: \")\n",
    "    if test_or_prod == \"Test\":\n",
    "        run_ETL(subset_size)\n",
    "    elif test_or_prod == \"Prod\":\n",
    "        subset_size = None\n",
    "        run_ETL(subset_size)\n",
    "    else:\n",
    "        print(\"Bad input\")\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47723898-c980-4c61-b91e-5c0408e8ed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test run of this code performs the construction of the KG on a random subset of 100 Conditions, 100 Interventions, and 100 Alternate Interventions from Clinical Trials.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is this a test run or the production of a new version of the KG? Enter Test for test, or Prod for production:  Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting download of Clinical Trial data as of 02_14_2024\n",
      "\n",
      "\n",
      "Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\n",
      "Please store the downloaded zip in the /data directory. This should be the only item besides the cache file, condition manual review file, and intervention manual review file, in the directory at this time.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type Done when done:  Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found at: \n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/9opmph4n5l7055moqnfu3n6kxnc0.zip\n",
      "Please make sure this the correct zip file from AACT\n",
      "Unzipping data into\n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/12_11_2023_extracted\n",
      "\n",
      "Approximate runtime for downloading or locating raw data: 0.0 hours, 0.0 minutes, 59.264199018478394 seconds\n",
      "MetaMap version < 2020, conduct mapping on terms after removing ascii characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% conditions mapped: 100%|| 55/55 [00:57<00:00,  1\n",
      "% interventions mapped: 100%|| 72/72 [01:34<00:00,\n",
      "% alternate_interventions mapped: 100%|| 56/56 [00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate runtime for mapping: 0.0 hours, 3.0 minutes, 22.62113380432129 seconds\n",
      "\n",
      "Mapping UMLS CURIEs and names back to clinical trials\n",
      "Scoring mappings for CONDITIONS\n",
      "Scoring mappings for INTERVENTIONS\n",
      "Auto-selecting high scoring CURIEs\n",
      "Auto-selecting CONDITIONS CURIEs\n",
      "Auto-selecting INTERVENTIONS CURIEs\n",
      "Approximate runtime for overall mapping: 0.0 hours, 7.0 minutes, 46.54691505432129 seconds\n"
     ]
    }
   ],
   "source": [
    "test_or_prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6a2d8500-2602-4cff-bd5c-c6f55b574d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_type</th>\n",
       "      <th>nct_id</th>\n",
       "      <th>original_clin_trial_term</th>\n",
       "      <th>modified_clin_trial_term</th>\n",
       "      <th>manually_selected_CURIE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>condition</td>\n",
       "      <td>NCT05353296</td>\n",
       "      <td>Insomnia</td>\n",
       "      <td>insomnia</td>\n",
       "      <td>C4554626|--|insomnia, ctcae 5.0|--|Finding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>condition</td>\n",
       "      <td>NCT03807284</td>\n",
       "      <td>Stroke</td>\n",
       "      <td>stroke</td>\n",
       "      <td>C4554100|--|stroke, ctcae|--|Finding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>condition</td>\n",
       "      <td>NCT05931185</td>\n",
       "      <td>Stroke</td>\n",
       "      <td>stroke</td>\n",
       "      <td>C4554100|--|stroke, ctcae|--|Finding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>condition</td>\n",
       "      <td>NCT01707095</td>\n",
       "      <td>Fatigue</td>\n",
       "      <td>fatigue</td>\n",
       "      <td>C4554645|--|fatigue, ctcae 5.0|--|Finding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>condition</td>\n",
       "      <td>NCT04899492</td>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>breast cancer</td>\n",
       "      <td>C0006142|--|malignant neoplasm of breast|--|Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49401</th>\n",
       "      <td>Oral Cancer</td>\n",
       "      <td>oral cancer</td>\n",
       "      <td>C0220641|--|lip and oral cavity carcinoma|--|N...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49402</th>\n",
       "      <td>Alcoholism</td>\n",
       "      <td>alcoholism</td>\n",
       "      <td>C0001973|--|alcoholic intoxication, chronic|--...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49403</th>\n",
       "      <td>Overweight and Obesity</td>\n",
       "      <td>overweight and obesity</td>\n",
       "      <td>C0028754|--|obesity|--|Disease or Syndrome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49404</th>\n",
       "      <td>Stress Disorders</td>\n",
       "      <td>stress disorders</td>\n",
       "      <td>C0038441|--|stress disorders, traumatic|--|Men...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49405</th>\n",
       "      <td>Breast Neoplasms</td>\n",
       "      <td>breast neoplasms</td>\n",
       "      <td>C1458155|--|mammary neoplasms|--|Neoplastic Pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49406 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    term_type                  nct_id  \\\n",
       "0                   condition             NCT05353296   \n",
       "1                   condition             NCT03807284   \n",
       "2                   condition             NCT05931185   \n",
       "3                   condition             NCT01707095   \n",
       "4                   condition             NCT04899492   \n",
       "...                       ...                     ...   \n",
       "49401             Oral Cancer             oral cancer   \n",
       "49402              Alcoholism              alcoholism   \n",
       "49403  Overweight and Obesity  overweight and obesity   \n",
       "49404        Stress Disorders        stress disorders   \n",
       "49405        Breast Neoplasms        breast neoplasms   \n",
       "\n",
       "                                original_clin_trial_term  \\\n",
       "0                                               Insomnia   \n",
       "1                                                 Stroke   \n",
       "2                                                 Stroke   \n",
       "3                                                Fatigue   \n",
       "4                                          Breast Cancer   \n",
       "...                                                  ...   \n",
       "49401  C0220641|--|lip and oral cavity carcinoma|--|N...   \n",
       "49402  C0001973|--|alcoholic intoxication, chronic|--...   \n",
       "49403         C0028754|--|obesity|--|Disease or Syndrome   \n",
       "49404  C0038441|--|stress disorders, traumatic|--|Men...   \n",
       "49405  C1458155|--|mammary neoplasms|--|Neoplastic Pr...   \n",
       "\n",
       "      modified_clin_trial_term  \\\n",
       "0                     insomnia   \n",
       "1                       stroke   \n",
       "2                       stroke   \n",
       "3                      fatigue   \n",
       "4                breast cancer   \n",
       "...                        ...   \n",
       "49401                      NaN   \n",
       "49402                      NaN   \n",
       "49403                      NaN   \n",
       "49404                      NaN   \n",
       "49405                      NaN   \n",
       "\n",
       "                                 manually_selected_CURIE  \n",
       "0             C4554626|--|insomnia, ctcae 5.0|--|Finding  \n",
       "1                   C4554100|--|stroke, ctcae|--|Finding  \n",
       "2                   C4554100|--|stroke, ctcae|--|Finding  \n",
       "3              C4554645|--|fatigue, ctcae 5.0|--|Finding  \n",
       "4      C0006142|--|malignant neoplasm of breast|--|Ne...  \n",
       "...                                                  ...  \n",
       "49401                                                NaN  \n",
       "49402                                                NaN  \n",
       "49403                                                NaN  \n",
       "49404                                                NaN  \n",
       "49405                                                NaN  \n",
       "\n",
       "[49406 rows x 5 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def compile_curies_for_trials(flag_and_path):\n",
    "    \n",
    "relevant_date = flag_and_path[\"date_string\"]\n",
    "\n",
    "conditions_autoselected_file = f\"{relevant_date}_conditions_autoselected.tsv\"\n",
    "conditions_autoselected = pd.read_csv(conditions_autoselected_file, sep=\"\\t\", on_bad_lines = 'warn')  # this is already mapped to nct_ids. Let's do this for the manually selected CURIEs now and add them  \n",
    "\n",
    "# get manually selected CURIEs\n",
    "conditions_manselected = pd.read_csv(\"conditions_manually_selected_cache.tsv\", sep=\"\\t\", on_bad_lines = 'warn')  # this is already mapped to nct_ids. Let's do this for the manually selected CURIEs now and add them  \n",
    "conditions_manselected\n",
    "# mapper_con = dict(zip(conditions_manselected['orig_con'], conditions_manselected['manually_selected_CURIE'])) # make a dict from the manual terms cache to map conditions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # map back to original terms    \n",
    "# conditions = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "# conditions_mapped = conditions.copy()\n",
    "# conditions_mapped['manually_selected_CURIE'] = conditions_mapped['downcase_name'].map(mapper_con)\n",
    "# conditions_mapped.drop([\"id\"], axis = 1, inplace = True)\n",
    "# # conditions_actually_mapped = conditions_mapped[~conditions_mapped['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "# conditions_mapped = conditions_mapped[~conditions_mapped['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "# conditions_mapped[\"term_type\"] = \"condition\"\n",
    "# conditions_mapped = conditions_mapped[[\"term_type\", \"nct_id\", \"name\", \"downcase_name\", \"manually_selected_CURIE\"]]\n",
    "# conditions_mapped.rename(columns = {'name':'original_clin_trial_term', 'downcase_name':'modified_clin_trial_term'}, inplace = True)\n",
    "# # with open('manually_selected_conditions_cache.tsv', 'a') as output:\n",
    "# #     conditions_mapped.to_csv(output, mode='a',sep=\"\\t\", index=False, header=output.tell()==0)\n",
    "\n",
    "# conditions_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "efe313af-88bf-4ee5-921f-c5debe05068b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Remove duplicate rows from cache '"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_clin_trial_term</th>\n",
       "      <th>modified_clin_trial_term</th>\n",
       "      <th>manually_selected_CURIE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cognitive Behavioral Therapy</td>\n",
       "      <td>cognitive behavioral therapy</td>\n",
       "      <td>C0009244|--|cognitive therapy|--|Therapeutic o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Saline</td>\n",
       "      <td>saline</td>\n",
       "      <td>C0036082|--|saline solution|--|Substance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Normal Saline Placebo</td>\n",
       "      <td>normal saline placebo</td>\n",
       "      <td>C1706408|--|placebo control|--|Research Activity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Placebo oral rinse</td>\n",
       "      <td>placebo oral rinse</td>\n",
       "      <td>C0032042|--|placebos|--|Therapeutic or Prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Adaptative treatment plan</td>\n",
       "      <td>adaptative treatment plan</td>\n",
       "      <td>C0599880|--|treatment plan|--|Intellectual Pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Mindfulness meditation</td>\n",
       "      <td>mindfulness meditation</td>\n",
       "      <td>C0814263|--|meditation therapy|--|Therapeutic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>biphasic insulin aspart 30</td>\n",
       "      <td>st101</td>\n",
       "      <td>C2003521|--|insulin aspart, insulin aspart pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>surgery</td>\n",
       "      <td>C0543467|--|operative surgical procedures|--|T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Lotensin 40 mg Tablets</td>\n",
       "      <td>lotensin 40 mg tablets</td>\n",
       "      <td>C0721436|--|lotensin|--|Organic Chemical|Pharm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>PET/MRI</td>\n",
       "      <td>pet/mri</td>\n",
       "      <td>C3641326|--|positron emission tomography and m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Acupuncture</td>\n",
       "      <td>acupuncture</td>\n",
       "      <td>C0001299|--|acupuncture therapy discipline|--|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>IL-12</td>\n",
       "      <td>il-12</td>\n",
       "      <td>C0123759|--|interleukin-12|--|Amino Acid, Pept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>radiation therapy</td>\n",
       "      <td>ascorbate</td>\n",
       "      <td>C1522449|--|therapeutic radiology procedure|--...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>MRI</td>\n",
       "      <td>mri</td>\n",
       "      <td>C0024485|--|magnetic resonance imaging|--|Diag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>radiotherapy</td>\n",
       "      <td>adefovir</td>\n",
       "      <td>C1522449|--|therapeutic radiology procedure|--...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>Corticosteroids</td>\n",
       "      <td>corticosteroids</td>\n",
       "      <td>C3540725|--|corticosteroid otologicals|--|Orga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>High flow nasal cannula</td>\n",
       "      <td>high flow nasal cannula</td>\n",
       "      <td>C0179574|--|nasal cannula|--|Medical Device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>Rehabilitation</td>\n",
       "      <td>rehabilitation</td>\n",
       "      <td>C0034991|--|rehabilitation therapy|--|Therapeu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>Blood Sample</td>\n",
       "      <td>blood sample</td>\n",
       "      <td>C0178913|--|blood specimen|--|Body Substance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>In vitro fertilization without surgery</td>\n",
       "      <td>in vitro fertilization without surgery</td>\n",
       "      <td>C0015915|--|fertilization in vitro|--|Therapeu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>Mindfulness Meditation</td>\n",
       "      <td>mindfulness meditation</td>\n",
       "      <td>C0814263|--|meditation therapy|--|Therapeutic ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    original_clin_trial_term  \\\n",
       "8               Cognitive Behavioral Therapy   \n",
       "38                                    Saline   \n",
       "113                    Normal Saline Placebo   \n",
       "140                       Placebo oral rinse   \n",
       "178                Adaptative treatment plan   \n",
       "220                   Mindfulness meditation   \n",
       "258               biphasic insulin aspart 30   \n",
       "345                                  Surgery   \n",
       "381                  Lotensin 40 mg Tablets   \n",
       "392                                  PET/MRI   \n",
       "405                              Acupuncture   \n",
       "436                                    IL-12   \n",
       "574                        radiation therapy   \n",
       "627                                      MRI   \n",
       "737                             radiotherapy   \n",
       "907                          Corticosteroids   \n",
       "1011                 High flow nasal cannula   \n",
       "1060                          Rehabilitation   \n",
       "1179                            Blood Sample   \n",
       "1272  In vitro fertilization without surgery   \n",
       "1347                  Mindfulness Meditation   \n",
       "\n",
       "                    modified_clin_trial_term  \\\n",
       "8               cognitive behavioral therapy   \n",
       "38                                    saline   \n",
       "113                    normal saline placebo   \n",
       "140                       placebo oral rinse   \n",
       "178                adaptative treatment plan   \n",
       "220                   mindfulness meditation   \n",
       "258                                    st101   \n",
       "345                                  surgery   \n",
       "381                  lotensin 40 mg tablets   \n",
       "392                                  pet/mri   \n",
       "405                              acupuncture   \n",
       "436                                    il-12   \n",
       "574                                ascorbate   \n",
       "627                                      mri   \n",
       "737                                 adefovir   \n",
       "907                          corticosteroids   \n",
       "1011                 high flow nasal cannula   \n",
       "1060                          rehabilitation   \n",
       "1179                            blood sample   \n",
       "1272  in vitro fertilization without surgery   \n",
       "1347                  mindfulness meditation   \n",
       "\n",
       "                                manually_selected_CURIE  \n",
       "8     C0009244|--|cognitive therapy|--|Therapeutic o...  \n",
       "38             C0036082|--|saline solution|--|Substance  \n",
       "113    C1706408|--|placebo control|--|Research Activity  \n",
       "140   C0032042|--|placebos|--|Therapeutic or Prevent...  \n",
       "178   C0599880|--|treatment plan|--|Intellectual Pro...  \n",
       "220   C0814263|--|meditation therapy|--|Therapeutic ...  \n",
       "258   C2003521|--|insulin aspart, insulin aspart pro...  \n",
       "345   C0543467|--|operative surgical procedures|--|T...  \n",
       "381   C0721436|--|lotensin|--|Organic Chemical|Pharm...  \n",
       "392   C3641326|--|positron emission tomography and m...  \n",
       "405   C0001299|--|acupuncture therapy discipline|--|...  \n",
       "436   C0123759|--|interleukin-12|--|Amino Acid, Pept...  \n",
       "574   C1522449|--|therapeutic radiology procedure|--...  \n",
       "627   C0024485|--|magnetic resonance imaging|--|Diag...  \n",
       "737   C1522449|--|therapeutic radiology procedure|--...  \n",
       "907   C3540725|--|corticosteroid otologicals|--|Orga...  \n",
       "1011        C0179574|--|nasal cannula|--|Medical Device  \n",
       "1060  C0034991|--|rehabilitation therapy|--|Therapeu...  \n",
       "1179       C0178913|--|blood specimen|--|Body Substance  \n",
       "1272  C0015915|--|fertilization in vitro|--|Therapeu...  \n",
       "1347  C0814263|--|meditation therapy|--|Therapeutic ...  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----     ------     GENERATE MANUALLY SELECTED CACHE     -----     ------  #\n",
    "#  --- --- --   CONDITIONS     --- --- --   #\n",
    "conditions = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "files = glob.glob(\"*.xlsx\")\n",
    "conditions_manselected_files = [i for i in files if \"conditions_manual_review\" in i if not i.startswith(\"~\")][0]  \n",
    "conditions_manselected = pd.read_excel(conditions_manselected_files)\n",
    "conditions_manselected.name.ffill(inplace=True)\n",
    "conditions_manselected.orig_con.ffill(inplace=True)\n",
    "conditions_manselected = conditions_manselected[~conditions_manselected['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "conditions_manselected.drop([\"curie_info\"], axis = 1, inplace = True)\n",
    "conditions_manselected.rename(columns = {'name':'original_clin_trial_term', 'orig_con':'modified_clin_trial_term'}, inplace = True)\n",
    "\n",
    "with open('manually_selected_conditions_cache.tsv', 'a') as output:\n",
    "    conditions_manselected.to_csv(output, mode='a',sep=\"\\t\", index=False, header=output.tell()==0)\n",
    "\"\"\" Remove duplicate rows from cache \"\"\"\n",
    "cache = pd.read_csv(\"manually_selected_conditions_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "cache = cache.drop_duplicates()\n",
    "cache.to_csv('manually_selected_conditions_cache.tsv', sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "\n",
    "#  --- --- --   INTERVENTIONS and Alternate INTERVENTIONS   --- --- --   #\n",
    "interventions = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "files = glob.glob(\"*.xlsx\")\n",
    "interventions_manselected_files = [i for i in files if \"interventions_manual_review\" in i if not i.startswith(\"~\")][0]  \n",
    "interventions_manselected = pd.read_excel(interventions_manselected_files)\n",
    "interventions_manselected.name.ffill(inplace=True)\n",
    "interventions_manselected.orig_int.ffill(inplace=True)\n",
    "interventions_manselected = interventions_manselected[~interventions_manselected['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "\n",
    "interventions_manselected.drop([\"curie_info\", \"description\"], axis = 1, inplace = True)\n",
    "interventions_manselected.rename(columns = {'name':'original_clin_trial_term', 'orig_int':'modified_clin_trial_term'}, inplace = True)\n",
    "\n",
    "with open('manually_selected_interventions_cache.tsv', 'a') as output:\n",
    "    interventions_manselected.to_csv(output, mode='a',sep=\"\\t\", index=False, header=output.tell()==0)\n",
    "\"\"\" Remove duplicate rows from cache \"\"\"\n",
    "cache = pd.read_csv(\"manually_selected_interventions_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "cache = cache.drop_duplicates()\n",
    "cache.to_csv('manually_selected_interventions_cache.tsv', sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "\n",
    "\n",
    "\n",
    "# mapper_con = dict(zip(conditions_manselected['orig_con'], conditions_manselected['manually_selected_CURIE'])) # make a dict from the manual terms cache to map conditions\n",
    "\n",
    "# # map back to original terms\n",
    "# conditions_mapped = conditions.copy()\n",
    "# conditions_mapped['manually_selected_CURIE'] = conditions_mapped['downcase_name'].map(mapper_con)\n",
    "# conditions_mapped.drop([\"id\"], axis = 1, inplace = True)\n",
    "# # conditions_actually_mapped = conditions_mapped[~conditions_mapped['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "# conditions_mapped = conditions_mapped[~conditions_mapped['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "# conditions_mapped[\"term_type\"] = \"condition\"\n",
    "# conditions_mapped = conditions_mapped[[\"term_type\", \"nct_id\", \"name\", \"downcase_name\", \"manually_selected_CURIE\"]]\n",
    "# conditions_mapped.rename(columns = {'name':'original_clin_trial_term', 'downcase_name':'modified_clin_trial_term'}, inplace = True)\n",
    "# with open('manually_selected_conditions_cache.tsv', 'a') as output:\n",
    "#     conditions_mapped.to_csv(output, mode='a',sep=\"\\t\", index=False, header=output.tell()==0)\n",
    "\n",
    "# \"\"\" Remove duplicate rows from cache \"\"\"\n",
    "# cache = pd.read_csv(\"manually_selected_conditions_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "# cache = cache.drop_duplicates()\n",
    "# cache.to_csv('manually_selected_conditions_cache.tsv', sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "\n",
    "# #  --- --- --   INTERVENTIONS and Alternate INTERVENTIONS   --- --- --   #\n",
    "# interventions = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "# interventions_alts = pd.read_csv(data_extracted + '/intervention_other_names.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "# files = glob.glob(\"*.xlsx\")\n",
    "# interventions_manselected_files = [i for i in files if \"interventions_manual_review\" in i if not i.startswith(\"~\")][0]  \n",
    "# interventions_manselected = pd.read_excel(interventions_manselected_files)\n",
    "# interventions_manselected.name.ffill(inplace=True)\n",
    "# interventions_manselected.orig_int.ffill(inplace=True)\n",
    "# interventions_manselected = interventions_manselected[~interventions_manselected['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "# mapper_int = dict(zip(interventions_manselected['orig_int'], interventions_manselected['manually_selected_CURIE'])) # make a dict from the manual terms cache to map conditions\n",
    "\n",
    "# # map back to original terms for both interventions and alternate interventions\n",
    "# interventions_alts = interventions_alts.merge(interventions[[\"id\", \"intervention_type\", \"description\"]], left_on='intervention_id', right_on='id', how='left') \n",
    "# interventions_alts.drop([\"id_x\", \"id_y\", \"intervention_id\"], axis = 1, inplace = True)\n",
    "# interventions.drop([\"id\"], axis = 1, inplace = True)\n",
    "# interventions[\"term_type\"] = \"intervention\"\n",
    "# interventions_alts[\"term_type\"] = \"alternate_intervention\"\n",
    "\n",
    "# interventions_mapped = pd.concat([interventions, interventions_alts], ignore_index=True) # all interventions\n",
    "# interventions_mapped[\"downcase_name\"] = interventions_mapped['name'].str.lower()\n",
    "# interventions_mapped['manually_selected_CURIE'] = interventions_mapped['downcase_name'].map(mapper_int)\n",
    "# # interventions_actually_mapped = interventions_mapped[~interventions_mapped['manually_selected_CURIE'].isnull()] # check if the interventions got mapped to any CURIEs\n",
    "# # alternate_interventions_actually_mapped = interventions_actually_mapped.loc[interventions_actually_mapped['term_type'] == \"alternate_intervention\"] # check if the alternate interventions got mapped too\n",
    "# interventions_mapped = interventions_mapped[~interventions_mapped['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "# interventions_mapped = interventions_mapped[[\"term_type\", \"nct_id\", \"intervention_type\", \"name\", \"downcase_name\", \"description\", \"manually_selected_CURIE\"]]\n",
    "# interventions_mapped.rename(columns = {'name':'original_clin_trial_term', 'downcase_name':'modified_clin_trial_term'}, inplace = True)\n",
    "\n",
    "# with open('manually_selected_interventions_cache.tsv', 'a') as output:\n",
    "#     interventions_mapped.to_csv(output, mode='a',sep=\"\\t\", index=False, header=output.tell()==0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24457599-6da3-4cf0-81f4-12c86c48446f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79952ad2-2e2a-499b-b949-b3c3af67c707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bcc21f-f7d3-4485-9866-18460e8d0dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b4e7dc-2639-4088-9720-412df6ea4114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ce327-ccea-41c1-9f81-0e022c0fdbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bb16a-1a17-40f4-b305-b8075153909a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc1c94-c7d0-4678-8891-93eb9183b599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc10df-f69d-457a-b745-71e2f4daf473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599855d-8f39-4421-85f1-f8baee80888e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261388d-8bf0-4487-bbea-9c2f77ab279a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15baff-a75f-4d94-a261-e3de733afd96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d0c75-8014-4220-992b-4bd8e888da95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f6e49-93a8-4e39-9943-645e1bdf5a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b780277-fd21-4f45-8bff-8d48bec3df93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765eded4-a23c-4c28-b7f6-1a2a8044f3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274cba7-fb3b-49df-8521-452b6f3760a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5d927-95b1-4726-8be9-0b2af77ddf22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f5d83b5c-7127-4865-a36d-2abca1b59698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nct_id</th>\n",
       "      <th>name</th>\n",
       "      <th>downcase_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69661551</td>\n",
       "      <td>NCT05901805</td>\n",
       "      <td>Sedentary Time</td>\n",
       "      <td>sedentary time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69661830</td>\n",
       "      <td>NCT05347784</td>\n",
       "      <td>Postoperative Complications</td>\n",
       "      <td>postoperative complications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69661930</td>\n",
       "      <td>NCT04946721</td>\n",
       "      <td>oGVHD</td>\n",
       "      <td>ogvhd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69662011</td>\n",
       "      <td>NCT04618432</td>\n",
       "      <td>Head and Neck Carcinoma</td>\n",
       "      <td>head and neck carcinoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69662100</td>\n",
       "      <td>NCT03898310</td>\n",
       "      <td>Incontinence</td>\n",
       "      <td>incontinence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824023</th>\n",
       "      <td>69056639</td>\n",
       "      <td>NCT01109394</td>\n",
       "      <td>Sarcoma</td>\n",
       "      <td>sarcoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824024</th>\n",
       "      <td>69056640</td>\n",
       "      <td>NCT01109394</td>\n",
       "      <td>Endocrine Tumors</td>\n",
       "      <td>endocrine tumors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824025</th>\n",
       "      <td>69056641</td>\n",
       "      <td>NCT01109394</td>\n",
       "      <td>Neuroblastoma</td>\n",
       "      <td>neuroblastoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824026</th>\n",
       "      <td>69056642</td>\n",
       "      <td>NCT01109394</td>\n",
       "      <td>Retinoblastoma</td>\n",
       "      <td>retinoblastoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824027</th>\n",
       "      <td>69056647</td>\n",
       "      <td>NCT01107340</td>\n",
       "      <td>Fracture of the Femoral Neck or Head</td>\n",
       "      <td>fracture of the femoral neck or head</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>824028 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       nct_id                                  name  \\\n",
       "0       69661551  NCT05901805                        Sedentary Time   \n",
       "1       69661830  NCT05347784           Postoperative Complications   \n",
       "2       69661930  NCT04946721                                 oGVHD   \n",
       "3       69662011  NCT04618432               Head and Neck Carcinoma   \n",
       "4       69662100  NCT03898310                          Incontinence   \n",
       "...          ...          ...                                   ...   \n",
       "824023  69056639  NCT01109394                               Sarcoma   \n",
       "824024  69056640  NCT01109394                      Endocrine Tumors   \n",
       "824025  69056641  NCT01109394                         Neuroblastoma   \n",
       "824026  69056642  NCT01109394                        Retinoblastoma   \n",
       "824027  69056647  NCT01107340  Fracture of the Femoral Neck or Head   \n",
       "\n",
       "                               downcase_name  \n",
       "0                             sedentary time  \n",
       "1                postoperative complications  \n",
       "2                                      ogvhd  \n",
       "3                    head and neck carcinoma  \n",
       "4                               incontinence  \n",
       "...                                      ...  \n",
       "824023                               sarcoma  \n",
       "824024                      endocrine tumors  \n",
       "824025                         neuroblastoma  \n",
       "824026                        retinoblastoma  \n",
       "824027  fracture of the femoral neck or head  \n",
       "\n",
       "[824028 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "# read in pipe-delimited files \n",
    "conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "conditions_mapped = conditions_df.copy()\n",
    "conditions_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c05b4a5c-c30b-4918-8d08-490a07535794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting download of Clinical Trial data as of 02_13_2024\n",
      "\n",
      "\n",
      "Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\n",
      "Please store the downloaded zip in the /data directory. This should be the only item besides the cache file, condition manual review file, and intervention manual review file, in the directory at this time.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type Done when done:  Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found at: \n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/9opmph4n5l7055moqnfu3n6kxnc0.zip\n",
      "Please make sure this the correct zip file from AACT\n",
      "Unzipping data into\n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/12_11_2023_extracted\n"
     ]
    }
   ],
   "source": [
    "subset_size = 200\n",
    "flag_and_path = get_raw_ct_data() # download raw data\n",
    "df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# relevant_date = flag_and_path[\"date_string\"]\n",
    "\n",
    "# conditions_autoselected_file = f\"{relevant_date}_conditions_autoselected.tsv\"\n",
    "# conditions_autoselected = pd.read_csv(conditions_autoselected_file, sep=\"\\t\", on_bad_lines = 'warn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "da29911c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_id</th>\n",
       "      <th>intervention_type</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>term_type</th>\n",
       "      <th>downcase_name</th>\n",
       "      <th>manually_selected_CURIE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>807256</th>\n",
       "      <td>NCT03505736</td>\n",
       "      <td>Procedure</td>\n",
       "      <td>MRI</td>\n",
       "      <td>Undergo CMR imaging</td>\n",
       "      <td>alternate_intervention</td>\n",
       "      <td>mri</td>\n",
       "      <td>C0024485|--|magnetic resonance imaging|--|Diagnostic Procedure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807822</th>\n",
       "      <td>NCT02318810</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Saline</td>\n",
       "      <td>Patients receive saline</td>\n",
       "      <td>alternate_intervention</td>\n",
       "      <td>saline</td>\n",
       "      <td>C0036082|--|saline solution|--|Substance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808546</th>\n",
       "      <td>NCT00331422</td>\n",
       "      <td>Procedure</td>\n",
       "      <td>surgery</td>\n",
       "      <td>Surgery - tumor specimen collected for extreme drug resistant assay (EDR) and A1 assays for analysis</td>\n",
       "      <td>alternate_intervention</td>\n",
       "      <td>surgery</td>\n",
       "      <td>C0543467|--|operative surgical procedures|--|Therapeutic or Preventive Procedure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809309</th>\n",
       "      <td>NCT05017714</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Saline</td>\n",
       "      <td>Aerosolized Novaferon, given 20 ug BID, daily for 7 days, plus Standard of Care.~Inhaled Saline (placebo), given BID, daily for 7 days, plus Standard of Care.</td>\n",
       "      <td>alternate_intervention</td>\n",
       "      <td>saline</td>\n",
       "      <td>C0036082|--|saline solution|--|Substance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809578</th>\n",
       "      <td>NCT00352118</td>\n",
       "      <td>Procedure</td>\n",
       "      <td>surgery</td>\n",
       "      <td>As appropriate, neck dissection.</td>\n",
       "      <td>alternate_intervention</td>\n",
       "      <td>surgery</td>\n",
       "      <td>C0543467|--|operative surgical procedures|--|Therapeutic or Preventive Procedure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809884</th>\n",
       "      <td>NCT00225147</td>\n",
       "      <td>Drug</td>\n",
       "      <td>saline</td>\n",
       "      <td>saline solution</td>\n",
       "      <td>alternate_intervention</td>\n",
       "      <td>saline</td>\n",
       "      <td>C0036082|--|saline solution|--|Substance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810350</th>\n",
       "      <td>NCT06160206</td>\n",
       "      <td>Procedure</td>\n",
       "      <td>MRI</td>\n",
       "      <td>Undergo MRI</td>\n",
       "      <td>alternate_intervention</td>\n",
       "      <td>mri</td>\n",
       "      <td>C0024485|--|magnetic resonance imaging|--|Diagnostic Procedure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810629</th>\n",
       "      <td>NCT00293202</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Saline</td>\n",
       "      <td>Hemodialysis patients will receive Saline by subcutaneous injection twice a week</td>\n",
       "      <td>alternate_intervention</td>\n",
       "      <td>saline</td>\n",
       "      <td>C0036082|--|saline solution|--|Substance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810676</th>\n",
       "      <td>NCT06018077</td>\n",
       "      <td>Behavioral</td>\n",
       "      <td>Blood Sample</td>\n",
       "      <td>Nutritional status of individuals will be evaluated with a 24-hour retrospective food consumption record and food consumption frequency questionnaire. After 8-12 hours of fasting, body composition analysis will be performed and within the scope of anthropometric measurements, body weight, height length, BMI measurements will be taken. Then, venous blood will be taken from the participants C-reactive protein (CRP), albumin and chemerin level will be analyzed.</td>\n",
       "      <td>alternate_intervention</td>\n",
       "      <td>blood sample</td>\n",
       "      <td>C0178913|--|blood specimen|--|Body Substance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810696</th>\n",
       "      <td>NCT06084845</td>\n",
       "      <td>Procedure</td>\n",
       "      <td>MRI</td>\n",
       "      <td>Undergo MRI</td>\n",
       "      <td>alternate_intervention</td>\n",
       "      <td>mri</td>\n",
       "      <td>C0024485|--|magnetic resonance imaging|--|Diagnostic Procedure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             nct_id intervention_type          name  \\\n",
       "807256  NCT03505736         Procedure           MRI   \n",
       "807822  NCT02318810              Drug        Saline   \n",
       "808546  NCT00331422         Procedure       surgery   \n",
       "809309  NCT05017714              Drug        Saline   \n",
       "809578  NCT00352118         Procedure       surgery   \n",
       "809884  NCT00225147              Drug        saline   \n",
       "810350  NCT06160206         Procedure           MRI   \n",
       "810629  NCT00293202              Drug        Saline   \n",
       "810676  NCT06018077        Behavioral  Blood Sample   \n",
       "810696  NCT06084845         Procedure           MRI   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                           description  \\\n",
       "807256                                                                                                                                                                                                                                                                                                                                                                                                                                                             Undergo CMR imaging   \n",
       "807822                                                                                                                                                                                                                                                                                                                                                                                                                                                         Patients receive saline   \n",
       "808546                                                                                                                                                                                                                                                                                                                                                                            Surgery - tumor specimen collected for extreme drug resistant assay (EDR) and A1 assays for analysis   \n",
       "809309                                                                                                                                                                                                                                                                                                                  Aerosolized Novaferon, given 20 ug BID, daily for 7 days, plus Standard of Care.~Inhaled Saline (placebo), given BID, daily for 7 days, plus Standard of Care.   \n",
       "809578                                                                                                                                                                                                                                                                                                                                                                                                                                                As appropriate, neck dissection.   \n",
       "809884                                                                                                                                                                                                                                                                                                                                                                                                                                                                 saline solution   \n",
       "810350                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Undergo MRI   \n",
       "810629                                                                                                                                                                                                                                                                                                                                                                                                Hemodialysis patients will receive Saline by subcutaneous injection twice a week   \n",
       "810676  Nutritional status of individuals will be evaluated with a 24-hour retrospective food consumption record and food consumption frequency questionnaire. After 8-12 hours of fasting, body composition analysis will be performed and within the scope of anthropometric measurements, body weight, height length, BMI measurements will be taken. Then, venous blood will be taken from the participants C-reactive protein (CRP), albumin and chemerin level will be analyzed.   \n",
       "810696                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Undergo MRI   \n",
       "\n",
       "                     term_type downcase_name  \\\n",
       "807256  alternate_intervention           mri   \n",
       "807822  alternate_intervention        saline   \n",
       "808546  alternate_intervention       surgery   \n",
       "809309  alternate_intervention        saline   \n",
       "809578  alternate_intervention       surgery   \n",
       "809884  alternate_intervention        saline   \n",
       "810350  alternate_intervention           mri   \n",
       "810629  alternate_intervention        saline   \n",
       "810676  alternate_intervention  blood sample   \n",
       "810696  alternate_intervention           mri   \n",
       "\n",
       "                                                                 manually_selected_CURIE  \n",
       "807256                    C0024485|--|magnetic resonance imaging|--|Diagnostic Procedure  \n",
       "807822                                          C0036082|--|saline solution|--|Substance  \n",
       "808546  C0543467|--|operative surgical procedures|--|Therapeutic or Preventive Procedure  \n",
       "809309                                          C0036082|--|saline solution|--|Substance  \n",
       "809578  C0543467|--|operative surgical procedures|--|Therapeutic or Preventive Procedure  \n",
       "809884                                          C0036082|--|saline solution|--|Substance  \n",
       "810350                    C0024485|--|magnetic resonance imaging|--|Diagnostic Procedure  \n",
       "810629                                          C0036082|--|saline solution|--|Substance  \n",
       "810676                                      C0178913|--|blood specimen|--|Body Substance  \n",
       "810696                    C0024485|--|magnetic resonance imaging|--|Diagnostic Procedure  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):  # more options can be specified also\n",
    "    display(test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_stats(df_dict, flag_and_path):\n",
    "    \"\"\" Report counts of conditions, interventions\"\"\"\n",
    "    relevant_date = flag_and_path[\"date_string\"] # get date\n",
    "    \n",
    "    total_conditions = df_dict[\"conditions\"].downcase_name\n",
    "    total_conditions = list(total_conditions.unique())\n",
    "    total_conditions = list(filter(None, total_conditions))\n",
    "    \n",
    "    orig_interventions = df_dict[\"interventions\"]\n",
    "    orig_interventions = orig_interventions['name'].str.lower()\n",
    "    orig_interventions = list(orig_interventions.unique())\n",
    "    orig_interventions = list(filter(None, orig_interventions))\n",
    "    \n",
    "    alt_interventions = df_dict[\"interventions_alts\"].alt_downcase_name\n",
    "    alt_interventions = list(alt_interventions.unique())\n",
    "    alt_interventions = list(filter(None, alt_interventions))\n",
    "    \n",
    "#     metamap_input = \"{}_metamap_output.tsv\".format(relevant_date)\n",
    "    \n",
    "#     \"\"\" Get the full names of the semantic types and replace the abbreviations with the full names \"\"\"\n",
    "#     metamapped = pd.read_csv(metamap_input, sep='\\t', index_col=False, header=0)\n",
    "\n",
    "    print(\"Clinical Trial Data from: {}\".format(relevant_date))\n",
    "    print(\"Total # of unique conditions : {}\".format(len(total_conditions)))\n",
    "    print(\"Total # of unique interventions : {}\".format(len(orig_interventions) + len(alt_interventions)))\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "103736d5-fae2-49c1-b197-b990ba70c70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_manually_selected_terms_to_cache():\n",
    "    # -----     ------     GENERATE MANUALLY SELECTED CACHE     -----     ------  #\n",
    "    #  --- --- --   CONDITIONS     --- --- --   #\n",
    "    conditions = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "    files = glob.glob(\"*.xlsx\")\n",
    "    conditions_manselected_files = [i for i in files if \"conditions_manual_review\" in i if not i.startswith(\"~\")][0]  \n",
    "    conditions_manselected = pd.read_excel(conditions_manselected_files)\n",
    "    conditions_manselected.name.ffill(inplace=True)\n",
    "    conditions_manselected.orig_con.ffill(inplace=True)\n",
    "    conditions_manselected = conditions_manselected[~conditions_manselected['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "    mapper_con = dict(zip(conditions_manselected['orig_con'], conditions_manselected['manually_selected_CURIE'])) # make a dict from the manual terms cache to map conditions\n",
    "\n",
    "    # map back to original terms\n",
    "    conditions_mapped = conditions.copy()\n",
    "    conditions_mapped['manually_selected_CURIE'] = conditions_mapped['downcase_name'].map(mapper_con)\n",
    "    conditions_mapped.drop([\"id\"], axis = 1, inplace = True)\n",
    "    # conditions_actually_mapped = conditions_mapped[~conditions_mapped['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "    conditions_mapped = conditions_mapped[~conditions_mapped['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "    conditions_mapped[\"term_type\"] = \"condition\"\n",
    "    conditions_mapped = conditions_mapped[[\"term_type\", \"nct_id\", \"name\", \"downcase_name\", \"manually_selected_CURIE\"]]\n",
    "    conditions_mapped.rename(columns = {'name':'original_clin_trial_term', 'downcase_name':'modified_clin_trial_term'}, inplace = True)\n",
    "    with open('conditions_manually_selected_cache.tsv', 'a') as output:\n",
    "        conditions_mapped.to_csv(output, mode='a',sep=\"\\t\", index=False, header=output.tell()==0)\n",
    "    \n",
    "    \"\"\" Remove duplicate rows from cache \"\"\"\n",
    "    cache = pd.read_csv(\"conditions_manually_selected_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    cache = cache.drop_duplicates()\n",
    "    cache.to_csv('conditions_manually_selected_cache.tsv', sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "\n",
    "    #  --- --- --   INTERVENTIONS and Alternate INTERVENTIONS   --- --- --   #\n",
    "    interventions = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    interventions_alts = pd.read_csv(data_extracted + '/intervention_other_names.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "    files = glob.glob(\"*.xlsx\")\n",
    "    interventions_manselected_files = [i for i in files if \"interventions_manual_review\" in i if not i.startswith(\"~\")][0]  \n",
    "    interventions_manselected = pd.read_excel(interventions_manselected_files)\n",
    "    interventions_manselected.name.ffill(inplace=True)\n",
    "    interventions_manselected.orig_int.ffill(inplace=True)\n",
    "    interventions_manselected = interventions_manselected[~interventions_manselected['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "    mapper_int = dict(zip(interventions_manselected['orig_int'], interventions_manselected['manually_selected_CURIE'])) # make a dict from the manual terms cache to map conditions\n",
    "\n",
    "    # map back to original terms for both interventions and alternate interventions\n",
    "    interventions_alts = interventions_alts.merge(interventions[[\"id\", \"intervention_type\", \"description\"]], left_on='intervention_id', right_on='id', how='left') \n",
    "    interventions_alts.drop([\"id_x\", \"id_y\", \"intervention_id\"], axis = 1, inplace = True)\n",
    "    interventions.drop([\"id\"], axis = 1, inplace = True)\n",
    "    interventions[\"term_type\"] = \"intervention\"\n",
    "    interventions_alts[\"term_type\"] = \"alternate_intervention\"\n",
    "\n",
    "    interventions_mapped = pd.concat([interventions, interventions_alts], ignore_index=True) # all interventions\n",
    "    interventions_mapped[\"downcase_name\"] = interventions_mapped['name'].str.lower()\n",
    "    interventions_mapped['manually_selected_CURIE'] = interventions_mapped['downcase_name'].map(mapper_int)\n",
    "    # interventions_actually_mapped = interventions_mapped[~interventions_mapped['manually_selected_CURIE'].isnull()] # check if the interventions got mapped to any CURIEs\n",
    "    # alternate_interventions_actually_mapped = interventions_actually_mapped.loc[interventions_actually_mapped['term_type'] == \"alternate_intervention\"] # check if the alternate interventions got mapped too\n",
    "    interventions_mapped = interventions_mapped[~interventions_mapped['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "    interventions_mapped = interventions_mapped[[\"term_type\", \"nct_id\", \"intervention_type\", \"name\", \"downcase_name\", \"description\", \"manually_selected_CURIE\"]]\n",
    "    interventions_mapped.rename(columns = {'name':'original_clin_trial_term', 'downcase_name':'modified_clin_trial_term'}, inplace = True)\n",
    "\n",
    "    with open('interventions_manually_selected_cache.tsv', 'a') as output:\n",
    "        interventions_mapped.to_csv(output, mode='a',sep=\"\\t\", index=False, header=output.tell()==0)\n",
    "    \n",
    "    \"\"\" Remove duplicate rows from cache \"\"\"\n",
    "    cache = pd.read_csv(\"interventions_manually_selected_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "    cache = cache.drop_duplicates()\n",
    "    cache.to_csv('interventions_manually_selected_cache.tsv', sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
